{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "afd997de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import entropy\n",
    "from scipy.signal import find_peaks\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix,f1_score, roc_auc_score\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,LabelEncoder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,CosineAnnealingLR\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn.model_selection import StratifiedKFold,cross_val_score\n",
    "import torch.optim as optim\n",
    "from catboost import CatBoostClassifier\n",
    "import neurokit2 as nk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available()) \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e1d24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(full_path):\n",
    "  dataset = []\n",
    "  labels = []\n",
    "  for file_name in os.listdir(full_path):\n",
    "    if file_name.endswith('.hea'):\n",
    "      continue\n",
    "    label = os.path.basename(os.path.dirname(full_path))\n",
    "    labels.append(label)\n",
    "    data = np.fromfile((os.path.join(full_path,file_name)),dtype = np.int16)\n",
    "    dataset.append(data)\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df['label'] = labels\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f473e246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\O'\n",
      "C:\\Users\\richk\\AppData\\Local\\Temp\\ipykernel_18184\\4126605693.py:2: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  df2 =create_db(full_path=\"C:\\\\Users\\\\richk\\OneDrive\\\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\\\2_Special_Form_VTTdP\\\\frag\")\n"
     ]
    }
   ],
   "source": [
    "df1 = create_db(full_path=\"C:\\\\Users\\\\richk\\\\OneDrive\\\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\\\1_Dangerous_VFL_VF\\\\frag\")\n",
    "df2 =create_db(full_path=\"C:\\\\Users\\\\richk\\OneDrive\\\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\\\2_Special_Form_VTTdP\\\\frag\")\n",
    "df3 = create_db(full_path=\"C:\\\\Users\\\\richk\\\\OneDrive\\\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\\\3_Threatening_VT\\\\frag\")\n",
    "df4 =create_db(full_path=\"C:\\\\Users\\\\richk\\\\OneDrive\\\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\\\4_Potential_Dangerous\\\\frag\")\n",
    "df5 = create_db(full_path=\"C:\\\\Users\\\\richk\\\\OneDrive\\\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\\\5_Supraventricular\\\\frag\")\n",
    "df6 =create_db(full_path=\"C:\\\\Users\\\\richk\\\\OneDrive\\\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\\\6_Sinus_rhythm\\\\frag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7cb5863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richk\\AppData\\Local\\Temp\\ipykernel_18184\\2629379934.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  full_dataset = full_dataset.replace(['1_Dangerous_VFL_VF','2_Special_Form_VTTdP','3_Threatening_VT','4_Potential_Dangerous','5_Supraventricular','6_Sinus_rhythm'],[0,1,2,3,4,5])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>712</th>\n",
       "      <th>713</th>\n",
       "      <th>714</th>\n",
       "      <th>715</th>\n",
       "      <th>716</th>\n",
       "      <th>717</th>\n",
       "      <th>718</th>\n",
       "      <th>719</th>\n",
       "      <th>720</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "      <td>178</td>\n",
       "      <td>175</td>\n",
       "      <td>172</td>\n",
       "      <td>171</td>\n",
       "      <td>173</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>168</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>198</td>\n",
       "      <td>202</td>\n",
       "      <td>204</td>\n",
       "      <td>202</td>\n",
       "      <td>199</td>\n",
       "      <td>194</td>\n",
       "      <td>191</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-191</td>\n",
       "      <td>-191</td>\n",
       "      <td>-192</td>\n",
       "      <td>-195</td>\n",
       "      <td>-200</td>\n",
       "      <td>-204</td>\n",
       "      <td>-202</td>\n",
       "      <td>-197</td>\n",
       "      <td>-189</td>\n",
       "      <td>-179</td>\n",
       "      <td>...</td>\n",
       "      <td>194</td>\n",
       "      <td>176</td>\n",
       "      <td>157</td>\n",
       "      <td>136</td>\n",
       "      <td>109</td>\n",
       "      <td>78</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-20</td>\n",
       "      <td>-48</td>\n",
       "      <td>-69</td>\n",
       "      <td>-86</td>\n",
       "      <td>-99</td>\n",
       "      <td>-110</td>\n",
       "      <td>-120</td>\n",
       "      <td>-130</td>\n",
       "      <td>-139</td>\n",
       "      <td>-149</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>-3</td>\n",
       "      <td>-29</td>\n",
       "      <td>-59</td>\n",
       "      <td>-89</td>\n",
       "      <td>-115</td>\n",
       "      <td>-135</td>\n",
       "      <td>-149</td>\n",
       "      <td>-158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89</td>\n",
       "      <td>99</td>\n",
       "      <td>107</td>\n",
       "      <td>112</td>\n",
       "      <td>117</td>\n",
       "      <td>121</td>\n",
       "      <td>126</td>\n",
       "      <td>132</td>\n",
       "      <td>137</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>-204</td>\n",
       "      <td>-209</td>\n",
       "      <td>-211</td>\n",
       "      <td>-213</td>\n",
       "      <td>-216</td>\n",
       "      <td>-217</td>\n",
       "      <td>-216</td>\n",
       "      <td>-214</td>\n",
       "      <td>-211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>217</td>\n",
       "      <td>224</td>\n",
       "      <td>227</td>\n",
       "      <td>228</td>\n",
       "      <td>229</td>\n",
       "      <td>230</td>\n",
       "      <td>232</td>\n",
       "      <td>234</td>\n",
       "      <td>235</td>\n",
       "      <td>235</td>\n",
       "      <td>...</td>\n",
       "      <td>140</td>\n",
       "      <td>133</td>\n",
       "      <td>127</td>\n",
       "      <td>121</td>\n",
       "      <td>115</td>\n",
       "      <td>110</td>\n",
       "      <td>104</td>\n",
       "      <td>96</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>-29</td>\n",
       "      <td>-30</td>\n",
       "      <td>-30</td>\n",
       "      <td>-30</td>\n",
       "      <td>-29</td>\n",
       "      <td>-28</td>\n",
       "      <td>-30</td>\n",
       "      <td>-33</td>\n",
       "      <td>-36</td>\n",
       "      <td>-35</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>-75</td>\n",
       "      <td>-75</td>\n",
       "      <td>-74</td>\n",
       "      <td>-75</td>\n",
       "      <td>-76</td>\n",
       "      <td>-75</td>\n",
       "      <td>-73</td>\n",
       "      <td>-71</td>\n",
       "      <td>-68</td>\n",
       "      <td>-66</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>-73</td>\n",
       "      <td>-75</td>\n",
       "      <td>-73</td>\n",
       "      <td>-70</td>\n",
       "      <td>-69</td>\n",
       "      <td>-71</td>\n",
       "      <td>-72</td>\n",
       "      <td>-73</td>\n",
       "      <td>-73</td>\n",
       "      <td>-72</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>-26</td>\n",
       "      <td>-23</td>\n",
       "      <td>-21</td>\n",
       "      <td>-20</td>\n",
       "      <td>-19</td>\n",
       "      <td>-18</td>\n",
       "      <td>-17</td>\n",
       "      <td>-18</td>\n",
       "      <td>-19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>-3</td>\n",
       "      <td>-9</td>\n",
       "      <td>-13</td>\n",
       "      <td>-16</td>\n",
       "      <td>-16</td>\n",
       "      <td>-17</td>\n",
       "      <td>-14</td>\n",
       "      <td>-11</td>\n",
       "      <td>-9</td>\n",
       "      <td>-8</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>-7</td>\n",
       "      <td>-5</td>\n",
       "      <td>-3</td>\n",
       "      <td>-3</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows √ó 722 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...  712  713  714  \\\n",
       "0     180  178  175  172  171  173  175  174  168  156  ...  192  198  202   \n",
       "1    -191 -191 -192 -195 -200 -204 -202 -197 -189 -179  ...  194  176  157   \n",
       "2     -20  -48  -69  -86  -99 -110 -120 -130 -139 -149  ...   18   -3  -29   \n",
       "3      89   99  107  112  117  121  126  132  137  142  ... -204 -209 -211   \n",
       "4     217  224  227  228  229  230  232  234  235  235  ...  140  133  127   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1011  -29  -30  -30  -30  -29  -28  -30  -33  -36  -35  ...   25   26   27   \n",
       "1012  -75  -75  -74  -75  -76  -75  -73  -71  -68  -66  ...   -2   -1    3   \n",
       "1013  -73  -75  -73  -70  -69  -71  -72  -73  -73  -72  ...   38   36   34   \n",
       "1014   16   19   25   27   27   24   21   19   20   21  ...  -26  -23  -21   \n",
       "1015   -3   -9  -13  -16  -16  -17  -14  -11   -9   -8  ...   -9   -7   -5   \n",
       "\n",
       "      715  716  717  718  719  720  label  \n",
       "0     204  202  199  194  191  188      0  \n",
       "1     136  109   78   45   11  -20      0  \n",
       "2     -59  -89 -115 -135 -149 -158      0  \n",
       "3    -213 -216 -217 -216 -214 -211      0  \n",
       "4     121  115  110  104   96   88      0  \n",
       "...   ...  ...  ...  ...  ...  ...    ...  \n",
       "1011   26   22   17   12    5    1      5  \n",
       "1012    4    5    3    1    1    3      5  \n",
       "1013   35   36   37   35   31   29      5  \n",
       "1014  -20  -19  -18  -17  -18  -19      5  \n",
       "1015   -3   -3   -2   -1    0    1      5  \n",
       "\n",
       "[1016 rows x 722 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = pd.concat([df1,df2,df3, df4, df5, df6], ignore_index=True)\n",
    "full_dataset = full_dataset.replace(['1_Dangerous_VFL_VF','2_Special_Form_VTTdP','3_Threatening_VT','4_Potential_Dangerous','5_Supraventricular','6_Sinus_rhythm'],[0,1,2,3,4,5])\n",
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "057d015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_signals(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_normalized = np.zeros_like(data, dtype=np.float32)\n",
    "    for i in range(data.shape[0]):\n",
    "        data_normalized[i] = scaler.fit_transform(data[i].reshape(-1, 1)).reshape(-1)\n",
    "    return data_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f850cf5",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "73563862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db1(full_path, numb=15):\n",
    "  combined_df = pd.DataFrame(columns=[i for i in range(numb+1)] + ['label'])\n",
    "  for file_name in os.listdir(full_path):\n",
    "    label = os.path.basename(os.path.dirname(full_path))\n",
    "    data = pd.read_csv(os.path.join(full_path,file_name), header=None)\n",
    "    data = data[0].tolist()\n",
    "    row = data[:len(data)] + [label]\n",
    "    try:\n",
    "        combined_df.loc[len(combined_df)] = row\n",
    "    except:\n",
    "        print(row, len(row))\n",
    "  return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "98edf75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = create_db1(r\"C:\\Users\\richk\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\1_Dangerous_VFL_VF\\15_2\")\n",
    "df12 = create_db1(r\"C:\\Users\\richk\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\2_Special_Form_VTTdP\\15_2\")\n",
    "df13 = create_db1(r\"C:\\Users\\richk\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\3_Threatening_VT\\15_2\")\n",
    "df14 = create_db1(r\"C:\\Users\\richk\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\4_Potential_Dangerous\\15_2\")\n",
    "df15 = create_db1(r\"C:\\Users\\richk\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\5_Supraventricular\\15_2\")\n",
    "df16 = create_db1(r\"C:\\Users\\richk\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ecg-fragment-database-for-the-exploration-of-dangerous-arrhythmia-1.0.0\\6_Sinus_rhythm\\15_2\")\n",
    "full_dataset_spec = pd.concat([df11,df12,df13, df14, df15, df16], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "97983b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500509</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.020108</td>\n",
       "      <td>0.353323</td>\n",
       "      <td>0.104229</td>\n",
       "      <td>0.004624</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>1_Dangerous_VFL_VF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.501034</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.446605</td>\n",
       "      <td>0.029388</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.005371</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>1_Dangerous_VFL_VF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.504155</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.197160</td>\n",
       "      <td>0.275896</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>1_Dangerous_VFL_VF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.501434</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.009609</td>\n",
       "      <td>0.013945</td>\n",
       "      <td>0.427695</td>\n",
       "      <td>0.023103</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.005770</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.005983</td>\n",
       "      <td>1_Dangerous_VFL_VF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.501845</td>\n",
       "      <td>0.016170</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.059468</td>\n",
       "      <td>0.388257</td>\n",
       "      <td>0.016360</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>1_Dangerous_VFL_VF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>0.532172</td>\n",
       "      <td>0.078659</td>\n",
       "      <td>0.163871</td>\n",
       "      <td>0.124378</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.004443</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.015258</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>0.012074</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.010889</td>\n",
       "      <td>0.009734</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.007114</td>\n",
       "      <td>6_Sinus_rhythm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>0.527665</td>\n",
       "      <td>0.083837</td>\n",
       "      <td>0.161463</td>\n",
       "      <td>0.140240</td>\n",
       "      <td>0.008756</td>\n",
       "      <td>0.006642</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.007970</td>\n",
       "      <td>0.008241</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.006855</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>6_Sinus_rhythm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>0.530357</td>\n",
       "      <td>0.116369</td>\n",
       "      <td>0.161673</td>\n",
       "      <td>0.090933</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.014657</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.008767</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>6_Sinus_rhythm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>0.588906</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>0.024792</td>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.046225</td>\n",
       "      <td>0.022378</td>\n",
       "      <td>0.017541</td>\n",
       "      <td>0.027613</td>\n",
       "      <td>0.039003</td>\n",
       "      <td>0.027436</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.037489</td>\n",
       "      <td>0.037989</td>\n",
       "      <td>0.037566</td>\n",
       "      <td>0.017335</td>\n",
       "      <td>6_Sinus_rhythm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>0.582093</td>\n",
       "      <td>0.036334</td>\n",
       "      <td>0.016375</td>\n",
       "      <td>0.044085</td>\n",
       "      <td>0.020337</td>\n",
       "      <td>0.038263</td>\n",
       "      <td>0.024644</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.021569</td>\n",
       "      <td>0.024308</td>\n",
       "      <td>0.022926</td>\n",
       "      <td>0.029258</td>\n",
       "      <td>0.042229</td>\n",
       "      <td>0.028766</td>\n",
       "      <td>0.021813</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>6_Sinus_rhythm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.500509  0.003944  0.001465  0.020108  0.353323  0.104229  0.004624   \n",
       "1     0.501034  0.000958  0.000307  0.004485  0.002104  0.446605  0.029388   \n",
       "2     0.504155  0.008499  0.001189  0.004717  0.003827  0.197160  0.275896   \n",
       "3     0.501434  0.008014  0.000634  0.009609  0.013945  0.427695  0.023103   \n",
       "4     0.501845  0.016170  0.000965  0.005679  0.059468  0.388257  0.016360   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1011  0.532172  0.078659  0.163871  0.124378  0.002418  0.004443  0.005249   \n",
       "1012  0.527665  0.083837  0.161463  0.140240  0.008756  0.006642  0.005331   \n",
       "1013  0.530357  0.116369  0.161673  0.090933  0.006801  0.003343  0.013240   \n",
       "1014  0.588906  0.009806  0.009397  0.024792  0.029349  0.046225  0.022378   \n",
       "1015  0.582093  0.036334  0.016375  0.044085  0.020337  0.038263  0.024644   \n",
       "\n",
       "             7         8         9        10        11        12        13  \\\n",
       "0     0.003921  0.001784  0.000782  0.001613  0.000220  0.000066  0.001069   \n",
       "1     0.000150  0.000578  0.000133  0.005371  0.001318  0.000123  0.000043   \n",
       "2     0.000279  0.000506  0.000080  0.000267  0.003288  0.000013  0.000036   \n",
       "3     0.000275  0.000387  0.000760  0.005770  0.000616  0.000255  0.000260   \n",
       "4     0.000474  0.000795  0.002133  0.001301  0.000734  0.000119  0.000774   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1011  0.015258  0.010414  0.012074  0.007973  0.010889  0.009734  0.009806   \n",
       "1012  0.012771  0.007970  0.008241  0.007373  0.006787  0.006855  0.006800   \n",
       "1013  0.014657  0.003945  0.009201  0.008982  0.008767  0.007886  0.008111   \n",
       "1014  0.017541  0.027613  0.039003  0.027436  0.027174  0.037489  0.037989   \n",
       "1015  0.024400  0.021569  0.024308  0.022926  0.029258  0.042229  0.028766   \n",
       "\n",
       "            14        15               label  \n",
       "0     0.002011  0.000333  1_Dangerous_VFL_VF  \n",
       "1     0.000631  0.006771  1_Dangerous_VFL_VF  \n",
       "2     0.000043  0.000045  1_Dangerous_VFL_VF  \n",
       "3     0.001259  0.005983  1_Dangerous_VFL_VF  \n",
       "4     0.003706  0.001220  1_Dangerous_VFL_VF  \n",
       "...        ...       ...                 ...  \n",
       "1011  0.005549  0.007114      6_Sinus_rhythm  \n",
       "1012  0.005388  0.003880      6_Sinus_rhythm  \n",
       "1013  0.008110  0.007626      6_Sinus_rhythm  \n",
       "1014  0.037566  0.017335      6_Sinus_rhythm  \n",
       "1015  0.021813  0.022600      6_Sinus_rhythm  \n",
       "\n",
       "[1016 rows x 17 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(full_dataset_spec) \n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "row_sums = df[numeric_columns].sum(axis=1)\n",
    "\n",
    "if (row_sums == 0).any():\n",
    "    raise ValueError(\"–ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–ø–µ–∫—Ç—Ä—ã –∏–º–µ—é—Ç –Ω—É–ª–µ–≤—É—é —Å—É–º–º—É –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–µ–π!\")\n",
    "\n",
    "\n",
    "df_normalized = df.copy()\n",
    "df_normalized[numeric_columns] = df[numeric_columns].div(row_sums, axis=0)\n",
    "df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3230ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMM(data):\n",
    "    return np.min(data), np.max(data), round(np.mean(data),2)\n",
    "\n",
    "def zero_cross(data):\n",
    "  ZC_count = 0\n",
    "  for i in range(len(data)-1):\n",
    "    if data[i] > 0  and data[i+1] < 0:\n",
    "      ZC_count += 1\n",
    "    if data[i] < 0 and data[i+1] > 0:\n",
    "      ZC_count += 1\n",
    "  return ZC_count\n",
    "\n",
    "def extract_frequency_features(data, fs):\n",
    "    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–∞\n",
    "    data = data - np.mean(data)\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º FFT\n",
    "    N = len(data)\n",
    "    spectrum = np.abs(fft(data))[:N // 2]   # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã\n",
    "    freqs = np.linspace(0, fs / 2, N // 2)\n",
    "\n",
    "    # –ù–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–µ–∫—Ç—Ä (–¥–ª—è —ç–Ω—Ç—Ä–æ–ø–∏–∏ –∏ centroid)\n",
    "    psd = spectrum ** 2\n",
    "    psd_norm = psd / np.sum(psd + 1e-12)  # +1e-12 ‚Äî —á—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∏—Ç—å –Ω–∞ 0\n",
    "\n",
    "    # üîπ 1. Spectral entropy\n",
    "    spec_entropy = entropy(psd_norm)\n",
    "\n",
    "    # üîπ 2. Dominant frequency\n",
    "    dom_freq = freqs[np.argmax(psd)]\n",
    "\n",
    "    # üîπ 3. Spectral centroid\n",
    "    centroid = np.sum(freqs * psd_norm)\n",
    "\n",
    "    return spec_entropy, dom_freq, centroid\n",
    "\n",
    "\n",
    "def heart_rate(data,fs):\n",
    "    peaks, _ = find_peaks(data, distance=fs*0.01, height=np.mean(data) + np.std(data))\n",
    "\n",
    "    rr_intervals_samples = np.diff(peaks)\n",
    "    rr_intervals_sec = rr_intervals_samples / fs\n",
    "    heart_rates = 60 / rr_intervals_sec\n",
    "\n",
    "    mean_hr = np.mean(heart_rates)\n",
    "\n",
    "    return round(mean_hr, 2)\n",
    "\n",
    "def preproc_file(full_path, fs = 360):\n",
    "  features =[]\n",
    "  for file_name in os.listdir(full_path):\n",
    "    if file_name.endswith('.hea'):\n",
    "      continue\n",
    "    data = np.fromfile((os.path.join(full_path,file_name)), dtype = np.int16)\n",
    "    min_v , max_v, mean_v = MMM(data)\n",
    "    HR = heart_rate(data,fs)\n",
    "    spec_entropy, dom_freq, centroid = extract_frequency_features(data, fs)\n",
    "    ZC = zero_cross(data)\n",
    "    features.append({\n",
    "        \"file\": file_name,\n",
    "        \"label\": os.path.basename(os.path.dirname(full_path)),\n",
    "        \"hr\": HR,\n",
    "        \"min\": min_v,\n",
    "        \"max\": max_v,\n",
    "        \"mean\": mean_v,\n",
    "        \"zero_cross\": ZC,\n",
    "        \"entropy\": spec_entropy,\n",
    "        \"dom_freq\": dom_freq,\n",
    "        \"centroid\": centroid\n",
    "    })\n",
    "  return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "35586ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_from_dataframe(df, fs=360):\n",
    "    features = []\n",
    "    \n",
    "    # –ò–¥–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º —Ç–∞–±–ª–∏—Ü—ã\n",
    "    for index, row in df.iterrows():\n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Å–∏–≥–Ω–∞–ª –∏–∑ –≤—Å–µ—Ö —Å—Ç–æ–ª–±—Ü–æ–≤, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º 'label'\n",
    "        signal = row.drop('label').values  # –°–∏–≥–Ω–∞–ª ‚Äî —ç—Ç–æ –≤—Å–µ, –∫—Ä–æ–º–µ 'label'\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "        min_v, max_v, mean_v = MMM(signal)\n",
    "        HR = heart_rate(signal, fs)\n",
    "        spec_entropy, dom_freq, centroid = extract_frequency_features(signal, fs)\n",
    "        ZC = zero_cross(signal)\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Å–ø–∏—Å–æ–∫\n",
    "        features.append({\n",
    "            \"hr\": HR,\n",
    "            \"min\": min_v,\n",
    "            \"max\": max_v,\n",
    "            \"mean\": mean_v,\n",
    "            \"zero_cross\": ZC,\n",
    "            \"entropy\": spec_entropy,\n",
    "            \"dom_freq\": dom_freq,\n",
    "            \"centroid\": centroid\n",
    "        })\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
    "    features_df = pd.DataFrame(features)\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –ª–µ–π–±–ª—ã –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ DataFrame\n",
    "    features_df['label'] = df['label']\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4bf88918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "processed_df = preproc_from_dataframe(full_dataset)\n",
    "processed_df = processed_df.drop('label',axis =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dec8edc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hr</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>zero_cross</th>\n",
       "      <th>entropy</th>\n",
       "      <th>dom_freq</th>\n",
       "      <th>centroid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1435.96</td>\n",
       "      <td>-299.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>-17.68</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.812203</td>\n",
       "      <td>4.011142</td>\n",
       "      <td>4.177260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1017.37</td>\n",
       "      <td>-228.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.021716</td>\n",
       "      <td>5.013928</td>\n",
       "      <td>5.329754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316.72</td>\n",
       "      <td>-210.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>21.59</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.415667</td>\n",
       "      <td>5.515320</td>\n",
       "      <td>5.264376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1194.15</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.734859</td>\n",
       "      <td>5.013928</td>\n",
       "      <td>5.537361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1010.44</td>\n",
       "      <td>-230.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>3.64</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.086808</td>\n",
       "      <td>4.512535</td>\n",
       "      <td>4.728101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>2004.91</td>\n",
       "      <td>-102.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>-11.24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.388714</td>\n",
       "      <td>1.504178</td>\n",
       "      <td>4.707126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1500.30</td>\n",
       "      <td>-83.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>-6.79</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.471035</td>\n",
       "      <td>1.504178</td>\n",
       "      <td>5.008852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>2231.71</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>-5.63</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.938972</td>\n",
       "      <td>2.506964</td>\n",
       "      <td>6.313203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>76.33</td>\n",
       "      <td>-357.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.506330</td>\n",
       "      <td>5.013928</td>\n",
       "      <td>12.115905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>77.17</td>\n",
       "      <td>-357.0</td>\n",
       "      <td>618.0</td>\n",
       "      <td>7.90</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.702971</td>\n",
       "      <td>5.013928</td>\n",
       "      <td>11.834435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           hr    min    max   mean  zero_cross   entropy  dom_freq   centroid\n",
       "0     1435.96 -299.0  238.0 -17.68        16.0  0.812203  4.011142   4.177260\n",
       "1     1017.37 -228.0  260.0   7.80        19.0  1.021716  5.013928   5.329754\n",
       "2      316.72 -210.0  320.0  21.59        20.0  1.415667  5.515320   5.264376\n",
       "3     1194.15 -233.0  207.0   1.44        19.0  1.734859  5.013928   5.537361\n",
       "4     1010.44 -230.0  235.0   3.64        17.0  2.086808  4.512535   4.728101\n",
       "...       ...    ...    ...    ...         ...       ...       ...        ...\n",
       "1011  2004.91 -102.0  139.0 -11.24        13.0  2.388714  1.504178   4.707126\n",
       "1012  1500.30  -83.0  117.0  -6.79        18.0  2.471035  1.504178   5.008852\n",
       "1013  2231.71  -78.0  135.0  -5.63        10.0  2.938972  2.506964   6.313203\n",
       "1014    76.33 -357.0  633.0  11.93        24.0  3.506330  5.013928  12.115905\n",
       "1015    77.17 -357.0  618.0   7.90        12.0  3.702971  5.013928  11.834435\n",
       "\n",
       "[1016 rows x 8 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = normalize_signals(processed_df.values) \n",
    "X_scaled_df = pd.DataFrame(processed_df.values,columns=processed_df.columns)\n",
    "X_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8fb2edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1016 entries, 0 to 1015\n",
      "Data columns (total 25 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   hr          1016 non-null   float64\n",
      " 1   min         1016 non-null   float64\n",
      " 2   max         1016 non-null   float64\n",
      " 3   mean        1016 non-null   float64\n",
      " 4   zero_cross  1016 non-null   float64\n",
      " 5   entropy     1016 non-null   float64\n",
      " 6   dom_freq    1016 non-null   float64\n",
      " 7   centroid    1016 non-null   float64\n",
      " 8   0           1016 non-null   float64\n",
      " 9   1           1016 non-null   float64\n",
      " 10  2           1016 non-null   float64\n",
      " 11  3           1016 non-null   float64\n",
      " 12  4           1016 non-null   float64\n",
      " 13  5           1016 non-null   float64\n",
      " 14  6           1016 non-null   float64\n",
      " 15  7           1016 non-null   float64\n",
      " 16  8           1016 non-null   float64\n",
      " 17  9           1016 non-null   float64\n",
      " 18  10          1016 non-null   float64\n",
      " 19  11          1016 non-null   float64\n",
      " 20  12          1016 non-null   float64\n",
      " 21  13          1016 non-null   float64\n",
      " 22  14          1016 non-null   float64\n",
      " 23  15          1016 non-null   float64\n",
      " 24  label       1016 non-null   object \n",
      "dtypes: float64(24), object(1)\n",
      "memory usage: 198.6+ KB\n"
     ]
    }
   ],
   "source": [
    "X_scaled_df = X_scaled_df.reset_index(drop=True)\n",
    "df_normalized = df_normalized.reset_index(drop=True)\n",
    "res_ = pd.concat([X_scaled_df, df_normalized], axis=1)\n",
    "res_['hr'] = res_['hr'].interpolate(method='linear')\n",
    "res_.info()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "000d2d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richk\\AppData\\Local\\Temp\\ipykernel_18184\\1916515944.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  res___= res_.replace(['1_Dangerous_VFL_VF','2_Special_Form_VTTdP','3_Threatening_VT','4_Potential_Dangerous','5_Supraventricular','6_Sinus_rhythm'],[0,1,2,3,4,5])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hr</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>zero_cross</th>\n",
       "      <th>entropy</th>\n",
       "      <th>dom_freq</th>\n",
       "      <th>centroid</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1435.96</td>\n",
       "      <td>-299.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>-17.68</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.812203</td>\n",
       "      <td>4.011142</td>\n",
       "      <td>4.177260</td>\n",
       "      <td>0.500509</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1017.37</td>\n",
       "      <td>-228.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.021716</td>\n",
       "      <td>5.013928</td>\n",
       "      <td>5.329754</td>\n",
       "      <td>0.501034</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.005371</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316.72</td>\n",
       "      <td>-210.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>21.59</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.415667</td>\n",
       "      <td>5.515320</td>\n",
       "      <td>5.264376</td>\n",
       "      <td>0.504155</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1194.15</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.734859</td>\n",
       "      <td>5.013928</td>\n",
       "      <td>5.537361</td>\n",
       "      <td>0.501434</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.005770</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.005983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1010.44</td>\n",
       "      <td>-230.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>3.64</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.086808</td>\n",
       "      <td>4.512535</td>\n",
       "      <td>4.728101</td>\n",
       "      <td>0.501845</td>\n",
       "      <td>0.016170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>2004.91</td>\n",
       "      <td>-102.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>-11.24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.388714</td>\n",
       "      <td>1.504178</td>\n",
       "      <td>4.707126</td>\n",
       "      <td>0.532172</td>\n",
       "      <td>0.078659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015258</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>0.012074</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.010889</td>\n",
       "      <td>0.009734</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.007114</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1500.30</td>\n",
       "      <td>-83.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>-6.79</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.471035</td>\n",
       "      <td>1.504178</td>\n",
       "      <td>5.008852</td>\n",
       "      <td>0.527665</td>\n",
       "      <td>0.083837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.007970</td>\n",
       "      <td>0.008241</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.006855</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>2231.71</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>-5.63</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.938972</td>\n",
       "      <td>2.506964</td>\n",
       "      <td>6.313203</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>0.116369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014657</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.008767</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>76.33</td>\n",
       "      <td>-357.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.506330</td>\n",
       "      <td>5.013928</td>\n",
       "      <td>12.115905</td>\n",
       "      <td>0.588906</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017541</td>\n",
       "      <td>0.027613</td>\n",
       "      <td>0.039003</td>\n",
       "      <td>0.027436</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.037489</td>\n",
       "      <td>0.037989</td>\n",
       "      <td>0.037566</td>\n",
       "      <td>0.017335</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>77.17</td>\n",
       "      <td>-357.0</td>\n",
       "      <td>618.0</td>\n",
       "      <td>7.90</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.702971</td>\n",
       "      <td>5.013928</td>\n",
       "      <td>11.834435</td>\n",
       "      <td>0.582093</td>\n",
       "      <td>0.036334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.021569</td>\n",
       "      <td>0.024308</td>\n",
       "      <td>0.022926</td>\n",
       "      <td>0.029258</td>\n",
       "      <td>0.042229</td>\n",
       "      <td>0.028766</td>\n",
       "      <td>0.021813</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           hr    min    max   mean  zero_cross   entropy  dom_freq   centroid  \\\n",
       "0     1435.96 -299.0  238.0 -17.68        16.0  0.812203  4.011142   4.177260   \n",
       "1     1017.37 -228.0  260.0   7.80        19.0  1.021716  5.013928   5.329754   \n",
       "2      316.72 -210.0  320.0  21.59        20.0  1.415667  5.515320   5.264376   \n",
       "3     1194.15 -233.0  207.0   1.44        19.0  1.734859  5.013928   5.537361   \n",
       "4     1010.44 -230.0  235.0   3.64        17.0  2.086808  4.512535   4.728101   \n",
       "...       ...    ...    ...    ...         ...       ...       ...        ...   \n",
       "1011  2004.91 -102.0  139.0 -11.24        13.0  2.388714  1.504178   4.707126   \n",
       "1012  1500.30  -83.0  117.0  -6.79        18.0  2.471035  1.504178   5.008852   \n",
       "1013  2231.71  -78.0  135.0  -5.63        10.0  2.938972  2.506964   6.313203   \n",
       "1014    76.33 -357.0  633.0  11.93        24.0  3.506330  5.013928  12.115905   \n",
       "1015    77.17 -357.0  618.0   7.90        12.0  3.702971  5.013928  11.834435   \n",
       "\n",
       "             0         1  ...         7         8         9        10  \\\n",
       "0     0.500509  0.003944  ...  0.003921  0.001784  0.000782  0.001613   \n",
       "1     0.501034  0.000958  ...  0.000150  0.000578  0.000133  0.005371   \n",
       "2     0.504155  0.008499  ...  0.000279  0.000506  0.000080  0.000267   \n",
       "3     0.501434  0.008014  ...  0.000275  0.000387  0.000760  0.005770   \n",
       "4     0.501845  0.016170  ...  0.000474  0.000795  0.002133  0.001301   \n",
       "...        ...       ...  ...       ...       ...       ...       ...   \n",
       "1011  0.532172  0.078659  ...  0.015258  0.010414  0.012074  0.007973   \n",
       "1012  0.527665  0.083837  ...  0.012771  0.007970  0.008241  0.007373   \n",
       "1013  0.530357  0.116369  ...  0.014657  0.003945  0.009201  0.008982   \n",
       "1014  0.588906  0.009806  ...  0.017541  0.027613  0.039003  0.027436   \n",
       "1015  0.582093  0.036334  ...  0.024400  0.021569  0.024308  0.022926   \n",
       "\n",
       "            11        12        13        14        15  label  \n",
       "0     0.000220  0.000066  0.001069  0.002011  0.000333      0  \n",
       "1     0.001318  0.000123  0.000043  0.000631  0.006771      0  \n",
       "2     0.003288  0.000013  0.000036  0.000043  0.000045      0  \n",
       "3     0.000616  0.000255  0.000260  0.001259  0.005983      0  \n",
       "4     0.000734  0.000119  0.000774  0.003706  0.001220      0  \n",
       "...        ...       ...       ...       ...       ...    ...  \n",
       "1011  0.010889  0.009734  0.009806  0.005549  0.007114      5  \n",
       "1012  0.006787  0.006855  0.006800  0.005388  0.003880      5  \n",
       "1013  0.008767  0.007886  0.008111  0.008110  0.007626      5  \n",
       "1014  0.027174  0.037489  0.037989  0.037566  0.017335      5  \n",
       "1015  0.029258  0.042229  0.028766  0.021813  0.022600      5  \n",
       "\n",
       "[1016 rows x 25 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res___= res_.replace(['1_Dangerous_VFL_VF','2_Special_Form_VTTdP','3_Threatening_VT','4_Potential_Dangerous','5_Supraventricular','6_Sinus_rhythm'],[0,1,2,3,4,5])\n",
    "res___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd9434",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "16620bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0030, 0.0139, 0.0059, 0.0076, 0.0094, 0.0050], device='cuda:0') [0.0029673590503570516, 0.013888888886959877, 0.005917159762963481, 0.007575757575183654, 0.009433962263260948, 0.0049999999997499996]\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # –¢–µ–Ω–∑–æ—Ä –≤–µ—Å–æ–≤ [n_classes]\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)  # [batch_size]\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            # –£–º–Ω–æ–∂–∞–µ–º CE –Ω–∞ alpha –î–û –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏\n",
    "            ce_loss = ce_loss * self.alpha[targets]\n",
    "        \n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "    \n",
    "support = [337, 72, 169, 132, 106, 200]  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "epsilon = 1e-8\n",
    "weights_cat = [1. / (s + epsilon) for s in support]\n",
    "weights = torch.tensor(weights_cat, dtype=torch.float32).to(device)\n",
    "print(weights,weights_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a8233639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Specnaz_gru_plus_norm(nn.Module):\n",
    "    def __init__(self, input_size=721, num_classes=6):\n",
    "        super(Specnaz_gru_plus_norm, self).__init__()\n",
    "\n",
    "        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π   \n",
    "        self.conv1 = nn.Conv1d(1,16, kernel_size=5,padding = 2)\n",
    "        self.layer_norm1 = nn.LayerNorm(16)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ —Å–≤–µ—Ä—Ç–∫–∏\n",
    "        self.gelu = nn.SiLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(16,32,kernel_size=5,padding =2 )\n",
    "        self.layer_norm2 = nn.LayerNorm(32) \n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "        self.pool = nn.AvgPool1d(kernel_size=2,stride =2 )\n",
    "\n",
    "        # GRU —Å–ª–æ–∏\n",
    "        self.gru1 = nn.GRU(input_size=32, hidden_size=64, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm3 = nn.LayerNorm(128)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "\n",
    "        self.gru2 = nn.GRU(input_size=128, hidden_size=256, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm4 = nn.LayerNorm(512)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ –≤—Ç–æ—Ä–æ–≥–æ qsdqdqdqd\n",
    "        self.dropout4 = nn.Dropout(0.1)\n",
    "\n",
    "        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        self.skip_con = nn.Linear(128, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  \n",
    "\n",
    "        x = self.conv1(x)   \n",
    "        x = x.permute(0, 2, 1)       \n",
    "        x = self.layer_norm1(x)      \n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "\n",
    "        x = self.conv2(x)   \n",
    "        x = x.permute(0, 2, 1)       \n",
    "        x = self.layer_norm2(x)      \n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.permute(0, 2, 1)     \n",
    "        x = self.pool(x)             \n",
    "        x = x.permute(0, 2, 1)       \n",
    "\n",
    "        x1, _ = self.gru1(x)\n",
    "        x1 = self.layer_norm3(x1)\n",
    "        x1= self.dropout3(x1) \n",
    "\n",
    "        x2, _ = self.gru2(x1)\n",
    "        x2 = self.layer_norm4(x2)\n",
    "        x2 = self.gelu(x2)\n",
    "        x2 = self.dropout4(x2)\n",
    "\n",
    "        x1 = self.skip_con(x1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = x[:, -1, :]\n",
    "        output = self.fc(x) \n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "bd388298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[255]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m val_loader_nn:\n\u001b[32m     99\u001b[39m     inputs, labels = inputs.to(device), labels.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m    102\u001b[39m     val_loss_epoch.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mSpecnaz_gru_plus_norm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     51\u001b[39m x1 = \u001b[38;5;28mself\u001b[39m.layer_norm3(x1)\n\u001b[32m     52\u001b[39m x1= \u001b[38;5;28mself\u001b[39m.dropout3(x1) \n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m x2, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgru2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m x2 = \u001b[38;5;28mself\u001b[39m.layer_norm4(x2)\n\u001b[32m     56\u001b[39m x2 = \u001b[38;5;28mself\u001b[39m.gelu(x2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1393\u001b[39m, in \u001b[36mGRU.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28mself\u001b[39m.check_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[32m   1392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1393\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1405\u001b[39m     result = _VF.gru(\n\u001b[32m   1406\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1407\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1414\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1415\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===\n",
    "\n",
    "\n",
    "X_nn = full_dataset.drop(columns=['label'])\n",
    "y_nn = full_dataset['label']\n",
    "X_cat = res___.drop(columns=['label'])\n",
    "y_cat = res___['label']\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_nn = label_encoder.fit_transform(y_nn)\n",
    "y_cat = label_encoder.transform(y_cat)\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train+val –∏ test\n",
    "X_train_val_nn, X_test_nn, y_train_val_nn, y_test_nn = train_test_split(\n",
    "    X_nn, y_nn, test_size=0.25, random_state=42, stratify=y_nn)\n",
    "X_train_val_cat, X_test_cat, y_train_val_cat, y_test_cat = train_test_split(\n",
    "    X_cat, y_cat, test_size=0.25, random_state=42, stratify=y_cat)\n",
    "\n",
    "# === –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===\n",
    "n_splits = 5\n",
    "num_epochs = 45\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === –•—Ä–∞–Ω–∏–ª–∏—â–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===\n",
    "all_nn_logits = []\n",
    "all_cat_logits = []\n",
    "all_labels = []\n",
    "f1_scores_per_fold_nn = []\n",
    "f1_scores_per_fold_cat = []\n",
    "\n",
    "# === –ì—Ä–∞—Ñ–∏–∫–∏ –ø–æ —Ñ–æ–ª–¥–∞–º ===\n",
    "all_train_losses = []\n",
    "all_f1_scores = []\n",
    "all_val_losses = []\n",
    "all_val_f1_scores = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val_nn, y_train_val_nn)):\n",
    "    y_val_fold = y_train_val_nn[val_idx]\n",
    "    values, counts = np.unique(y_val_fold, return_counts=True)\n",
    "    print(f\"\\nüîÅ Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ NN –¥–∞–Ω–Ω—ã—Ö\n",
    "    X_train_np_nn = X_train_val_nn.iloc[train_idx].values\n",
    "    X_val_np_nn = X_train_val_nn.iloc[val_idx].values\n",
    "    y_train_np_nn = y_train_val_nn[train_idx].astype(np.int64)\n",
    "    y_val_np_nn = y_train_val_nn[val_idx].astype(np.int64)\n",
    "\n",
    "    X_train_np_nn = normalize_signals(X_train_np_nn)\n",
    "    X_val_np_nn = normalize_signals(X_val_np_nn)\n",
    "\n",
    "    X_train_tensor_nn = torch.tensor(X_train_np_nn, dtype=torch.float32)\n",
    "    y_train_tensor_nn = torch.tensor(y_train_np_nn, dtype=torch.long)\n",
    "    X_val_tensor_nn = torch.tensor(X_val_np_nn, dtype=torch.float32)\n",
    "    y_val_tensor_nn = torch.tensor(y_val_np_nn, dtype=torch.long)\n",
    "\n",
    "    train_loader_nn = DataLoader(TensorDataset(X_train_tensor_nn, y_train_tensor_nn), batch_size=batch_size, shuffle=True)\n",
    "    val_loader_nn = DataLoader(TensorDataset(X_val_tensor_nn, y_val_tensor_nn), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = Specnaz_gru_plus_norm(input_size=721, num_classes=6).to(device)\n",
    "    # criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)\n",
    "    criterion = FocalLoss(alpha=weights,gamma=1)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.0002)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=25, T_mult=1, eta_min=1e-4\n",
    "    )\n",
    "\n",
    "    # === –î–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ ===\n",
    "    train_losses = []\n",
    "    f1_scores = []\n",
    "    val_losses_per_epoch = []\n",
    "    val_f1_per_epoch = []\n",
    "    lrs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss_epoch = []\n",
    "        for inputs, labels in train_loader_nn:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_epoch.append(loss.item())\n",
    "        scheduler.step()\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # === –í–∞–ª–∏–¥–∞—Ü–∏—è ===\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        val_loss_epoch = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader_nn:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss_epoch.append(loss.item())\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = np.mean(train_loss_epoch)\n",
    "        avg_val_loss = np.mean(val_loss_epoch)\n",
    "        val_f1_nn = f1_score(val_targets, val_preds, average='weighted')\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses_per_epoch.append(avg_val_loss)\n",
    "        f1_scores.append(val_f1_nn)\n",
    "        val_f1_per_epoch.append(val_f1_nn)\n",
    "    plt.plot(lrs)\n",
    "    plt.title(\"Learning Rate Schedule\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"LR\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    all_train_losses.append(train_losses)\n",
    "    all_val_losses.append(val_losses_per_epoch)\n",
    "    all_f1_scores.append(val_f1_per_epoch)\n",
    "    # === –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è ===\n",
    "    val_logits = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in val_loader_nn:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            val_logits.append(probs.cpu())\n",
    "\n",
    "    nn_fold_logits = torch.cat(val_logits).numpy()\n",
    "    all_nn_logits.append(nn_fold_logits)\n",
    "    \n",
    "    # === CatBoost ===\n",
    "    X_train_np_cat = X_train_val_cat.iloc[train_idx].values\n",
    "    X_val_np_cat = X_train_val_cat.iloc[val_idx].values\n",
    "    y_train_np_cat = y_train_val_cat[train_idx]\n",
    "    y_val_np_cat = y_train_val_cat[val_idx]\n",
    "\n",
    "    cat_model = CatBoostClassifier(verbose=0, iterations=2000, early_stopping_rounds=50,loss_function = 'MultiClass',eval_metric='MultiClass',random_seed=42)\n",
    "    cat_model.fit(X_train_np_cat, y_train_np_cat, eval_set=(X_val_np_cat, y_val_np_cat))\n",
    "    cat_probs = cat_model.predict_proba(X_val_np_cat)\n",
    "    cat_preds = np.argmax(cat_probs, axis=1)\n",
    "    cat_f1 = f1_score(y_val_np_cat, cat_preds, average='weighted')\n",
    "    all_cat_logits.append(cat_probs)\n",
    "\n",
    "    all_labels.append(y_val_np_nn)\n",
    "    f1_scores_per_fold_nn.append(val_f1_nn)\n",
    "    f1_scores_per_fold_cat.append(cat_f1)\n",
    "    torch.save(model.state_dict(), f\"nn_model_fold{fold}.pth\")\n",
    "    cat_model.save_model(f\"catboost_model_fold{fold}.cbm\")\n",
    "    print(f\"NN Report Fold {fold+1}:\\n\", classification_report(y_val_np_nn, np.argmax(nn_fold_logits, axis=1), digits=4))\n",
    "    print(f\"CatBoost Report Fold {fold+1}:\\n\", classification_report(y_val_np_cat, np.argmax(cat_probs, axis=1), digits=4))\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)  # –î–≤–∞ –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∞\n",
    "    plt.plot(all_train_losses[fold], label='Train Loss')\n",
    "    plt.plot(all_val_losses[fold], label='Val Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Fold {fold+1} - Loss per Epoch\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(all_f1_scores[fold], label='F1 Score', color='green')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 \")\n",
    "    plt.title(f\"Fold {fold+1} - F1 per Epoch\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3e8ef6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9189    1.0000    0.9577        34\n",
      "           1     0.6667    0.8571    0.7500         7\n",
      "           2     0.9231    0.7059    0.8000        17\n",
      "           3     0.8000    0.6154    0.6957        13\n",
      "           4     0.6364    0.6364    0.6364        11\n",
      "           5     0.7273    0.8000    0.7619        20\n",
      "\n",
      "    accuracy                         0.8137       102\n",
      "   macro avg     0.7787    0.7691    0.7669       102\n",
      "weighted avg     0.8191    0.8137    0.8107       102\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8718    1.0000    0.9315        34\n",
      "           1     0.5556    0.7143    0.6250         7\n",
      "           2     0.8333    0.5882    0.6897        17\n",
      "           3     1.0000    0.7692    0.8696        13\n",
      "           4     0.9091    0.9091    0.9091        11\n",
      "           5     0.9048    0.9500    0.9268        20\n",
      "\n",
      "    accuracy                         0.8627       102\n",
      "   macro avg     0.8458    0.8218    0.8253       102\n",
      "weighted avg     0.8705    0.8627    0.8589       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_logits_nn_list = []\n",
    "X_test_nn_np = normalize_signals(X_test_nn.values)\n",
    "X_test_tensor = torch.tensor(X_test_nn_np, dtype=torch.float32).to(device)\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    model = Specnaz_gru_plus_norm(input_size=721, num_classes=6).to(device)\n",
    "    model.load_state_dict(torch.load(f\"nn_model_fold{fold}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    fold_logits = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_test_tensor), batch_size):\n",
    "            batch = X_test_tensor[i:i+batch_size]\n",
    "            output = model(batch)\n",
    "            probs = torch.softmax(output, dim=1)\n",
    "            fold_logits.append(probs.cpu())\n",
    "\n",
    "    fold_logits = torch.cat(fold_logits).numpy()\n",
    "    test_logits_nn_list.append(fold_logits)\n",
    "nn_probs = np.mean(test_logits_nn_list, axis=0)\n",
    "\n",
    "cat_test_probs_list     = []\n",
    "for fold in range(n_splits):\n",
    "    model_cat = CatBoostClassifier()\n",
    "    model_cat.load_model(f\"catboost_model_fold{fold}.cbm\")\n",
    "    probs = model_cat.predict_proba(X_test_cat.values)\n",
    "    cat_test_probs_list.append(probs)\n",
    "cat_probs = np.mean(cat_test_probs_list, axis=0)\n",
    "\n",
    "nn_preds = np.argmax(nn_probs, axis=1)\n",
    "cat_preds = np.argmax(cat_probs, axis=1)\n",
    "# === –ú–µ—Ç–∫–∏ ===\n",
    "print(\"Meta Test Classification Report:\")\n",
    "print(classification_report(y_test_cat, nn_preds, digits=4))\n",
    "print(classification_report(y_test_cat, cat_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "00bd626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meta_features(probs):\n",
    "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π\"\"\"\n",
    "    eps = 1e-8\n",
    "    top1 = np.max(probs, axis=1, keepdims=True)\n",
    "    top2 = np.partition(probs, -2, axis=1)[:, -2][:, None]\n",
    "    top2_gap = (top1 - top2) / (top1 + eps)  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞\n",
    "    \n",
    "    entropy = -np.sum(probs * np.log(probs + eps), axis=1, keepdims=True)\n",
    "    max_entropy = np.log(probs.shape[1])  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è\n",
    "    norm_entropy = entropy / max_entropy  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è [0,1]\n",
    "    \n",
    "    uncertainty = 1 - top1  # –ú–µ—Ä–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "    \n",
    "    return np.concatenate([\n",
    "        probs,                 # –ò—Å—Ö–æ–¥–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "        top2,\n",
    "        top2_gap,\n",
    "        norm_entropy,\n",
    "        uncertainty\n",
    "    ], axis=1)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "meta_nn = [extract_meta_features(logits) for logits in all_nn_logits]\n",
    "meta_cat = [extract_meta_features(logits) for logits in all_cat_logits]\n",
    "\n",
    "# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ —Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º –º–æ–¥–µ–ª–µ–π\n",
    "X_meta = np.concatenate([\n",
    "    np.concatenate(meta_nn, axis=0),\n",
    "    np.concatenate(meta_cat, axis=0) \n",
    "], axis=1)\n",
    "y_meta = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3bc7554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ (–∞–Ω—Å–∞–º–±–ª—å), —Å—Ä–µ–¥–Ω–∏–π F1=0.7642,–ª—É—á—à–∏–π(0.8006)\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ (–∞–Ω—Å–∞–º–±–ª—å), —Å—Ä–µ–¥–Ω–∏–π F1=0.7834,–ª—É—á—à–∏–π(0.8219)\n"
     ]
    }
   ],
   "source": [
    "def choose_best_or_ensemble(val_f1_per_fold, threshold=0.075): \n",
    "    best_fold_idx = np.argmax(val_f1_per_fold)\n",
    "    best_f1 = val_f1_per_fold[best_fold_idx] \n",
    "    mean_f1 = np.mean(val_f1_per_fold)\n",
    "    \n",
    "    # –ï—Å–ª–∏ –ª—É—á—à–∏–π —Ñ–æ–ª–¥ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ª—É—á—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ (–Ω–∞ threshold), —Ç–æ –≤—ã–±–∏—Ä–∞–µ–º –µ–≥–æ\n",
    "    if best_f1 - mean_f1 > threshold:\n",
    "        print(f\"–í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (—Ñ–æ–ª–¥ {best_fold_idx}) —Å F1={best_f1:.4f},—Å—Ä–µ–¥–Ω–∏–π({mean_f1:.4f})\")\n",
    "        return 'best_model', best_fold_idx\n",
    "    else:\n",
    "        print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ (–∞–Ω—Å–∞–º–±–ª—å), —Å—Ä–µ–¥–Ω–∏–π F1={mean_f1:.4f},–ª—É—á—à–∏–π({best_f1:.4f})\")\n",
    "        return 'ensemble', None\n",
    "    \n",
    "mode_nn, best_fold_nn_idx = choose_best_or_ensemble(f1_scores_per_fold_nn)\n",
    "mode_cat, best_fold_cat_idx = choose_best_or_ensemble(f1_scores_per_fold_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b2b8c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meta_test(mode_nn,mode_cat,best_fold_nn_idx,best_fold_cat_idx,X_test_nn,X_test_cat,device):\n",
    "    if mode_nn == 'best_model':\n",
    "        model_nn = Specnaz_gru_plus_norm(721,num_classes=6).to(device)\n",
    "        model_nn.load_state_dict(torch.load( f\"nn_model_fold{best_fold_nn_idx}.pth\"))\n",
    "        model_nn.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(normalize_signals(X_test_nn.values),dtype=torch.float32).to(device)\n",
    "            outputs = model_nn(inputs)\n",
    "            nn_probs = torch.softmax(outputs,dim=1).cpu().numpy()\n",
    "    else:\n",
    "        test_logits_nn_list = []\n",
    "        X_test_nn_np = normalize_signals(X_test_nn.values)\n",
    "        X_test_tensor = torch.tensor(X_test_nn_np, dtype=torch.float32).to(device)\n",
    "\n",
    "        for fold in range(n_splits):\n",
    "            model = Specnaz_gru_plus_norm(input_size=721, num_classes=6).to(device)\n",
    "            model.load_state_dict(torch.load(f\"nn_model_fold{fold}.pth\"))\n",
    "            model.eval()\n",
    "\n",
    "            fold_logits = []\n",
    "            with torch.no_grad():\n",
    "                for i in range(0, len(X_test_tensor), batch_size):\n",
    "                    batch = X_test_tensor[i:i+batch_size]\n",
    "                    output = model(batch)\n",
    "                    probs = torch.softmax(output, dim=1)\n",
    "                    fold_logits.append(probs.cpu())\n",
    "\n",
    "            fold_logits = torch.cat(fold_logits).numpy()\n",
    "            test_logits_nn_list.append(fold_logits)\n",
    "        nn_probs = np.mean(test_logits_nn_list, axis=0)\n",
    "    \n",
    "    if mode_cat == 'best_model':\n",
    "        model_cat = CatBoostClassifier()\n",
    "        model_cat.load_model(f\"catboost_model_fold{best_fold_cat_idx}.cbm\")\n",
    "        cat_probs = cat_model.predict_proba(X_test_cat)\n",
    "    else:\n",
    "        cat_test_probs_list     = []\n",
    "        for fold in range(n_splits):\n",
    "            model_cat = CatBoostClassifier()\n",
    "            model_cat.load_model(f\"catboost_model_fold{fold}.cbm\")\n",
    "            probs = model_cat.predict_proba(X_test_cat.values)\n",
    "            cat_test_probs_list.append(probs)\n",
    "        cat_probs = np.mean(cat_test_probs_list, axis=0)\n",
    "    nn_meta = extract_meta_features(nn_probs)\n",
    "    cat_meta = extract_meta_features(cat_probs)\n",
    "    return np.concatenate([nn_meta,cat_meta],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1a68ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_notfix(nn.Module):\n",
    "    def __init__(self, input=20, output=6):\n",
    "        super(meta_notfix, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),           \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(256, output) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "cf6a5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 86.0246\n",
      "Epoch [1/20], Validation Loss: 8.2793, Validation Accuracy: 0.80%, F1-score: 0.8090\n",
      "Epoch [6/20], Train Loss: 42.8390\n",
      "Epoch [6/20], Validation Loss: 3.7865, Validation Accuracy: 0.85%, F1-score: 0.8466\n",
      "Epoch [11/20], Train Loss: 37.3334\n",
      "Epoch [11/20], Validation Loss: 2.9952, Validation Accuracy: 0.86%, F1-score: 0.8563\n",
      "Epoch [16/20], Train Loss: 33.3053\n",
      "Epoch [16/20], Validation Loss: 2.8119, Validation Accuracy: 0.86%, F1-score: 0.8563\n",
      "Epoch [20/20], Train Loss: 32.6946\n",
      "Epoch [20/20], Validation Loss: 2.6174, Validation Accuracy: 0.86%, F1-score: 0.8563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9032    0.9333    0.9180        30\n",
      "           1     0.6667    0.5714    0.6154         7\n",
      "           2     0.7857    0.7333    0.7586        15\n",
      "           3     0.8000    1.0000    0.8889        12\n",
      "           4     0.9000    0.9000    0.9000        10\n",
      "           5     0.9375    0.8333    0.8824        18\n",
      "\n",
      "    accuracy                         0.8587        92\n",
      "   macro avg     0.8322    0.8286    0.8272        92\n",
      "weighted avg     0.8590    0.8587    0.8563        92\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# –û–±–µ—Ä–Ω–∏ X_meta –∏ y_meta –≤ —Ç–µ–Ω–∑–æ—Ä—ã\n",
    "X_meta_train, X_meta_val, y_meta_train, y_meta_val = train_test_split(\n",
    "    X_meta, y_meta, test_size=0.1, random_state=42, stratify=y_meta\n",
    ")\n",
    "X_meta_train_tensor = torch.tensor(X_meta_train, dtype=torch.float32)\n",
    "y_meta_train_tensor = torch.tensor(y_meta_train, dtype=torch.long)\n",
    "X_meta_val_tensor = torch.tensor(X_meta_val, dtype=torch.float32)\n",
    "y_meta_val_tensor = torch.tensor(y_meta_val, dtype=torch.long)\n",
    "\n",
    "meta_train_dataset = TensorDataset(X_meta_train_tensor, y_meta_train_tensor)\n",
    "meta_val_dataset = TensorDataset(X_meta_val_tensor, y_meta_val_tensor)\n",
    "\n",
    "meta_train_loader = DataLoader(meta_train_dataset, batch_size=16, shuffle=True,drop_last=False)\n",
    "meta_val_loader = DataLoader(meta_val_dataset, batch_size=16, shuffle=False,drop_last=False)\n",
    "# –û–±—ä—è–≤–∏ –º–æ–¥–µ–ª—å\n",
    "meta_model = meta_notfix(input=20, output=6).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = FocalLoss(gamma=2.5)\n",
    "optimizer = torch.optim.AdamW(meta_model.parameters(), lr=7e-5)\n",
    "\n",
    "\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_model_state = None\n",
    "num_epochs_meta = 20\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "for epoch in range(num_epochs_meta):\n",
    "    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞\n",
    "    meta_model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in meta_train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = meta_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "    meta_model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in meta_val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = meta_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            val_preds.append(probs.cpu().numpy())\n",
    "            val_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    val_preds = np.concatenate(val_preds, axis=0)\n",
    "    val_labels = np.concatenate(val_labels, axis=0)\n",
    "    val_preds_classes = np.argmax(val_preds, axis=1)\n",
    "    val_accuracy = (val_preds_classes == val_labels).mean()\n",
    "    val_f1 = f1_score(val_labels, val_preds_classes, average='weighted')\n",
    "\n",
    "    train_losses.append(total_train_loss)\n",
    "    val_losses.append(total_val_loss)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model_state = meta_model.state_dict()\n",
    "\n",
    "    if epoch%5 == 0 or epoch == num_epochs_meta-1:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs_meta}], Train Loss: {total_train_loss:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs_meta}], Validation Loss: {total_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%, F1-score: {val_f1:.4f}\")\n",
    "print(classification_report(val_labels, val_preds_classes, digits=4))\n",
    "\n",
    "epochs = range(1, num_epochs_meta + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train & Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# F1 Score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, val_f1_scores, label='Validation F1 Score', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Validation F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "cdd67540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9651    0.9881    0.9765        84\n",
      "           1     0.8095    0.9444    0.8718        18\n",
      "           2     0.9487    0.8810    0.9136        42\n",
      "           3     0.9667    0.8788    0.9206        33\n",
      "           4     0.9630    0.9630    0.9630        27\n",
      "           5     0.9608    0.9800    0.9703        50\n",
      "\n",
      "    accuracy                         0.9488       254\n",
      "   macro avg     0.9356    0.9392    0.9360       254\n",
      "weighted avg     0.9505    0.9488    0.9487       254\n",
      "\n",
      "Confusion Matrix:\n",
      " [[83  1  0  0  0  0]\n",
      " [ 1 17  0  0  0  0]\n",
      " [ 2  3 37  0  0  0]\n",
      " [ 0  0  2 29  1  1]\n",
      " [ 0  0  0  0 26  1]\n",
      " [ 0  0  0  1  0 49]]\n"
     ]
    }
   ],
   "source": [
    "X_metat = make_meta_test(mode_nn,mode_cat,best_fold_nn_idx,best_fold_cat_idx,X_test_nn,X_test_cat,device)\n",
    "meta_model.load_state_dict(best_model_state)\n",
    "meta_model.eval()\n",
    "X_meta_test_tensor = torch.tensor(X_metat, dtype=torch.float32).to(device)\n",
    "meta_preds = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_meta_test_tensor), batch_size):\n",
    "        batch = X_meta_test_tensor[i:i+batch_size]\n",
    "        out = meta_model(batch)\n",
    "        probs = torch.softmax(out, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1)\n",
    "        meta_preds.extend(pred.cpu().numpy())\n",
    "\n",
    "meta_preds = np.array(meta_preds)\n",
    "\n",
    "# === –ú–µ—Ç–∫–∏ ===\n",
    "print(\"Meta Test Classification Report:\")\n",
    "print(classification_report(y_test_cat, meta_preds, digits=4))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_cat, meta_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
